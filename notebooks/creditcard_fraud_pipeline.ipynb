{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07624c3",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44609480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abel/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 08:21:59,180 - creditcard_fraud_pipeline - INFO - ================================================================================\n",
      "2026-01-31 08:21:59,181 - creditcard_fraud_pipeline - INFO - CREDIT CARD FRAUD DETECTION PIPELINE INITIALIZED\n",
      "2026-01-31 08:21:59,181 - creditcard_fraud_pipeline - INFO - ================================================================================\n",
      "2026-01-31 08:21:59,181 - creditcard_fraud_pipeline - INFO - CREDIT CARD FRAUD DETECTION PIPELINE INITIALIZED\n",
      "2026-01-31 08:21:59,181 - creditcard_fraud_pipeline - INFO - ================================================================================\n",
      "âœ“ All modules imported successfully\n",
      "âœ“ Project root: /home/abel/Desktop/Copied_with_better_precison/fraud-detection-for-e-commerce-and-bank-transactions\n",
      "âœ“ Data directory: /home/abel/Desktop/Copied_with_better_precison/fraud-detection-for-e-commerce-and-bank-transactions/data/raw\n",
      "âœ“ All modules imported successfully\n",
      "âœ“ Project root: /home/abel/Desktop/Copied_with_better_precison/fraud-detection-for-e-commerce-and-bank-transactions\n",
      "âœ“ Data directory: /home/abel/Desktop/Copied_with_better_precison/fraud-detection-for-e-commerce-and-bank-transactions/data/raw\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelConfig' object has no attribute 'models_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Project root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Data directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_config\u001b[38;5;241m.\u001b[39mcreditcard_data_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Models directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelConfig' object has no attribute 'models_dir'"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from src.config import data_config, model_config\n",
    "from src.logger import setup_logger\n",
    "from src.data_loader import DataLoader\n",
    "from src.feature_engineer import CreditCardFeatureEngineer\n",
    "from src.preprocessor import DataPreprocessor\n",
    "from src.model_trainer import ModelTrainer\n",
    "from src.model_evaluator import ModelEvaluator\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger('creditcard_fraud_pipeline', 'creditcard_fraud_pipeline.log')\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"CREDIT CARD FRAUD DETECTION PIPELINE INITIALIZED\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")\n",
    "print(f\"âœ“ Project root: {project_root}\")\n",
    "print(f\"âœ“ Data directory: {data_config.creditcard_data_path.parent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81093e55",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "data_loader = DataLoader(config=data_config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CREDIT CARD FRAUD DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load credit card data\n",
    "creditcard_data = data_loader.load_creditcard_data()\n",
    "print(f\"\\nâœ“ Loaded {len(creditcard_data):,} credit card transactions\")\n",
    "print(f\"  - Fraud cases: {creditcard_data['Class'].sum():,} ({creditcard_data['Class'].mean()*100:.4f}%)\")\n",
    "print(f\"  - Normal cases: {(creditcard_data['Class']==0).sum():,} ({(creditcard_data['Class']==0).mean()*100:.4f}%)\")\n",
    "print(f\"  - Features: {creditcard_data.shape[1]}\")\n",
    "print(f\"  - PCA Features (V1-V28): 28 anonymized features\")\n",
    "print(f\"  - Time: Transaction time in seconds\")\n",
    "print(f\"  - Amount: Transaction amount\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ DATA LOADING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2403721",
   "metadata": {},
   "source": [
    "### Data Overview Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class imbalance and transaction patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Class distribution\n",
    "cc_counts = creditcard_data['Class'].value_counts()\n",
    "axes[0].bar(['Normal', 'Fraud'], cc_counts.values, color=['#3498db', '#e67e22'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[0].set_title('Credit Card Transaction Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "for i, v in enumerate(cc_counts.values):\n",
    "    axes[0].text(i, v + 1000, f'{v:,}\\n({v/len(creditcard_data)*100:.4f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Amount distribution by class\n",
    "axes[1].hist(creditcard_data[creditcard_data['Class']==0]['Amount'], bins=50, alpha=0.7, \n",
    "             label='Normal', color='#3498db', edgecolor='black')\n",
    "axes[1].hist(creditcard_data[creditcard_data['Class']==1]['Amount'], bins=50, alpha=0.7, \n",
    "             label='Fraud', color='#e67e22', edgecolor='black')\n",
    "axes[1].set_xlabel('Transaction Amount', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Transaction Amount Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_xlim([0, 500])  # Focus on typical range\n",
    "\n",
    "# Time distribution\n",
    "time_hours = creditcard_data['Time'] / 3600  # Convert to hours\n",
    "axes[2].scatter(time_hours[creditcard_data['Class']==0], \n",
    "                creditcard_data[creditcard_data['Class']==0]['Amount'], \n",
    "                alpha=0.1, s=1, label='Normal', color='#3498db')\n",
    "axes[2].scatter(time_hours[creditcard_data['Class']==1], \n",
    "                creditcard_data[creditcard_data['Class']==1]['Amount'], \n",
    "                alpha=0.8, s=20, label='Fraud', color='#e67e22', edgecolor='black', linewidth=0.5)\n",
    "axes[2].set_xlabel('Time (hours)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Amount', fontsize=12)\n",
    "axes[2].set_title('Fraud Pattern Over Time', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_ylim([0, 1000])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "imbalance_ratio = cc_counts[0]/cc_counts[1]\n",
    "print(f\"\\nEXTREME CLASS IMBALANCE: {imbalance_ratio:.0f}:1 ratio\")\n",
    "print(f\"   â†’ Will handle with SMOTE during preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581cf4b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = CreditCardFeatureEngineer()\n",
    "\n",
    "# Engineer features\n",
    "creditcard_data_engineered = feature_engineer.engineer_features(creditcard_data, fit=True)\n",
    "\n",
    "print(f\"\\nâœ“ Feature engineering completed\")\n",
    "print(f\"  - Original features: {creditcard_data.shape[1]}\")\n",
    "print(f\"  - Engineered features: {creditcard_data_engineered.shape[1]}\")\n",
    "print(f\"  - New features added: {creditcard_data_engineered.shape[1] - creditcard_data.shape[1]}\")\n",
    "\n",
    "# Display new features\n",
    "new_features = [col for col in creditcard_data_engineered.columns if col not in creditcard_data.columns]\n",
    "if new_features:\n",
    "    print(f\"\\nNew features created:\")\n",
    "    for i, feat in enumerate(new_features, 1):\n",
    "        print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f46f17",
   "metadata": {},
   "source": [
    "### Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature correlations with target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Top 15 features by correlation\n",
    "cc_numeric = creditcard_data_engineered.select_dtypes(include=[np.number])\n",
    "target_col = 'Class' if 'Class' in cc_numeric.columns else 'class'\n",
    "cc_corr = cc_numeric.corr()[target_col].abs().sort_values(ascending=False)[1:16]\n",
    "\n",
    "axes[0].barh(range(len(cc_corr)), cc_corr.values, color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_yticks(range(len(cc_corr)))\n",
    "axes[0].set_yticklabels(cc_corr.index, fontsize=9)\n",
    "axes[0].set_xlabel('Absolute Correlation with Fraud', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Top 15 Features by Correlation', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(cc_corr.values):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Correlation heatmap of top features\n",
    "top_features = cc_corr.index[:10].tolist() + [target_col]\n",
    "corr_matrix = cc_numeric[top_features].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn_r', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[1],\n",
    "            annot_kws={'size': 8})\n",
    "axes[1].set_title('Top 10 Features Correlation Matrix', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Feature correlation analysis complete\")\n",
    "print(\"\\nNote: V1-V28 are PCA-transformed features (anonymized for privacy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c4965",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963669a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data\n",
    "target_col = 'Class' if 'Class' in creditcard_data_engineered.columns else 'class'\n",
    "X = creditcard_data_engineered.drop(columns=[target_col])\n",
    "y = creditcard_data_engineered[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=data_config.test_size,\n",
    "    random_state=data_config.random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train):,} samples\")\n",
    "print(f\"  - Fraud: {y_train.sum():,} ({y_train.mean()*100:.4f}%)\")\n",
    "print(f\"  - Normal: {(y_train==0).sum():,} ({(y_train==0).mean()*100:.4f}%)\")\n",
    "print(f\"\\nTest set: {len(X_test):,} samples\")\n",
    "print(f\"  - Fraud: {y_test.sum():,} ({y_test.mean()*100:.4f}%)\")\n",
    "print(f\"  - Normal: {(y_test==0).sum():,} ({(y_test==0).mean()*100:.4f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ DATA SPLIT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cec79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING: SCALING & SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize preprocessor with SMOTE\n",
    "preprocessor = DataPreprocessor(use_smote=True)\n",
    "\n",
    "print(f\"\\nBefore SMOTE: {len(X_train):,} samples\")\n",
    "print(f\"  Class distribution: {(y_train==0).sum():,} Normal | {y_train.sum():,} Fraud\")\n",
    "print(f\"  Imbalance ratio: {(y_train==0).sum() / y_train.sum():.0f}:1\")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed, y_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nAfter SMOTE: {len(X_train_processed):,} samples\")\n",
    "print(f\"  Class distribution: {(y_train_processed==0).sum():,} Normal | {y_train_processed.sum():,} Fraud\")\n",
    "print(f\"  - Fraud: {y_train_processed.sum():,} ({y_train_processed.mean()*100:.2f}%)\")\n",
    "print(f\"  - Normal: {(y_train_processed==0).sum():,} ({(y_train_processed==0).mean()*100:.2f}%)\")\n",
    "print(f\"\\nâœ“ Class balance achieved!\")\n",
    "print(f\"âœ“ Training samples increased by {len(X_train_processed) - len(X_train):,} ({(len(X_train_processed)/len(X_train) - 1)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545967b9",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train three models: Logistic Regression, Random Forest, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ce7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FRAUD DETECTION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define models to train\n",
    "model_types = ['logistic_regression', 'random_forest', 'xgboost']\n",
    "models = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training: {model_type.replace('_', ' ').title()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    trainer = ModelTrainer(model_type=model_type)\n",
    "    trainer.train(X_train_processed, y_train_processed)\n",
    "    \n",
    "    # Store trained model\n",
    "    models[model_type] = trainer\n",
    "    \n",
    "    print(f\"\\nâœ“ {model_type.replace('_', ' ').title()} training complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ ALL MODELS TRAINED ({len(models)} models)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3555390",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation with all metrics including AUC-PR (critical for extreme imbalance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(threshold=0.5)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, trainer in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_name.replace('_', ' ').title()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = trainer.predict_proba(X_test_processed)\n",
    "    y_pred = trainer.predict(X_test_processed)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluator.evaluate(y_test, y_pred_proba, y_pred)\n",
    "    results[model_name] = metrics\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"\\n{'Performance Metrics':^80}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"  Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision:          {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:             {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:           {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  ROC-AUC:            {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  AUC-PR:             {metrics['average_precision']:.4f} â­\")\n",
    "    print(f\"  Specificity:        {metrics['specificity']:.4f}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8ec99",
   "metadata": {},
   "source": [
    "### Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [k.replace('_', ' ').title() for k in results.keys()],\n",
    "    'Accuracy': [v['accuracy'] for v in results.values()],\n",
    "    'Precision': [v['precision'] for v in results.values()],\n",
    "    'Recall': [v['recall'] for v in results.values()],\n",
    "    'F1-Score': [v['f1_score'] for v in results.values()],\n",
    "    'ROC-AUC': [v['roc_auc'] for v in results.values()],\n",
    "    'AUC-PR': [v['average_precision'] for v in results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model = comparison.loc[comparison['F1-Score'].idxmax(), 'Model']\n",
    "best_f1 = comparison['F1-Score'].max()\n",
    "best_recall = comparison.loc[comparison['F1-Score'].idxmax(), 'Recall']\n",
    "best_auc_pr = comparison.loc[comparison['F1-Score'].idxmax(), 'AUC-PR']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")\n",
    "print(f\"   Recall:   {best_recall:.4f} (catches {best_recall*100:.1f}% of fraud)\")\n",
    "print(f\"   AUC-PR:   {best_auc_pr:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620b25c",
   "metadata": {},
   "source": [
    "### Threshold Simulation (Recall vs Threshold)\n",
    "Explore how lowering the classification threshold increases recall at the cost of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD SIMULATION: RECALL VS THRESHOLD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pick a model to simulate (use best model if available)\n",
    "selected_model_key = None\n",
    "if 'best_model_key' in globals():\n",
    "    selected_model_key = best_model_key\n",
    "elif 'results' in globals() and isinstance(results, dict) and len(results) > 0:\n",
    "    selected_model_key = next(iter(results.keys()))\n",
    "else:\n",
    "    selected_model_key = None\n",
    "\n",
    "if selected_model_key is None:\n",
    "    print(\"\\nâš ï¸ No trained model found. Run training and evaluation cells first.\")\n",
    "else:\n",
    "    trainer = models[selected_model_key]\n",
    "    y_true = y_test\n",
    "    # Get predicted probabilities for positive class\n",
    "    y_proba = trainer.predict_proba(X_test_processed)\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba = y_proba[:, 1]\n",
    "\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_proba >= t).astype(int)\n",
    "        recall_t = recall_score(y_true, y_pred_t)\n",
    "        precision_t = precision_score(y_true, y_pred_t, zero_division=0)\n",
    "        rows.append({\"threshold\": t, \"recall\": recall_t, \"precision\": precision_t})\n",
    "\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "    print(\"\\nRecall/Precision by Threshold:\")\n",
    "    print(df_thr.to_string(index=False))\n",
    "\n",
    "    # Plot recall vs threshold\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(df_thr[\"threshold\"], df_thr[\"recall\"], marker=\"o\", label=\"Recall\")\n",
    "    plt.plot(df_thr[\"threshold\"], df_thr[\"precision\"], marker=\"o\", label=\"Precision\")\n",
    "    plt.axvline(0.5, color=\"gray\", linestyle=\"--\", label=\"Threshold = 0.50\")\n",
    "    plt.title(f\"Threshold Trade-off ({selected_model_key.replace('_', ' ').title()})\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Threshold simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1302313",
   "metadata": {},
   "source": [
    "### Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance bar charts\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'AUC-PR']\n",
    "colors = ['#e67e22', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    values = comparison[metric].values\n",
    "    bars = ax.bar(comparison['Model'], values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f'{metric}', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.suptitle('Credit Card Fraud Detection: Model Performance Comparison', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b466f13",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, trainer) in enumerate(models.items()):\n",
    "    y_pred = trainer.predict(X_test_processed)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'],\n",
    "                cbar_kws={'label': 'Count'}, annot_kws={'size': 14, 'weight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name.replace(\"_\", \" \").title()}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
    "    \n",
    "    # Add percentages\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total = cm.sum()\n",
    "    axes[idx].text(0.5, -0.15, f'TN: {tn:,} ({tn/total*100:.2f}%)', \n",
    "                   ha='center', transform=axes[idx].transAxes, fontsize=9)\n",
    "    axes[idx].text(1.5, -0.15, f'FP: {fp:,} ({fp/total*100:.2f}%)', \n",
    "                   ha='center', transform=axes[idx].transAxes, fontsize=9)\n",
    "\n",
    "plt.suptitle('Confusion Matrices', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de3a82",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a74d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['#e67e22', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for idx, (model_name, trainer) in enumerate(models.items()):\n",
    "    y_pred_proba = trainer.predict_proba(X_test_processed)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[idx], lw=2.5, \n",
    "            label=f'{model_name.replace(\"_\", \" \").title()} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Credit Card Fraud Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98418559",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves (Most Important for Extreme Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceed56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['#e67e22', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for idx, (model_name, trainer) in enumerate(models.items()):\n",
    "    y_pred_proba = trainer.predict_proba(X_test_processed)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(recall, precision, color=colors[idx], lw=2.5, \n",
    "            label=f'{model_name.replace(\"_\", \" \").title()} (AP = {avg_precision:.3f})')\n",
    "\n",
    "# Baseline\n",
    "baseline = y_test.sum() / len(y_test)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', lw=2, label=f'Baseline (AP = {baseline:.4f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "plt.title('Precision-Recall Curves - Credit Card Fraud Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"best\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUC-PR is CRITICAL for extreme class imbalance ({}:1 ratio)\".format(int(imbalance_ratio)))\n",
    "print(\"   â†’ ROC-AUC can be misleadingly high when classes are highly imbalanced\")\n",
    "print(\"   â†’ AUC-PR focuses exclusively on fraud class performance\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33948f51",
   "metadata": {},
   "source": [
    "## 7. Feature Importance & Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47929f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get best model\n",
    "best_model_key = best_model.lower().replace(' ', '_')\n",
    "best_trainer = models[best_model_key]\n",
    "\n",
    "if hasattr(best_trainer.model, 'feature_importances_'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{best_model} - Top 15 Most Important Features\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    importance = best_trainer.model.feature_importances_\n",
    "    feature_names = X_train_processed.columns if hasattr(X_train_processed, 'columns') else [f'Feature_{i}' for i in range(len(importance))]\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    \n",
    "    print(\"\\n\" + feature_importance_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'], \n",
    "             color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'], fontsize=10)\n",
    "    plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features - {best_model}', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(feature_importance_df['Importance']):\n",
    "        plt.text(v + 0.001, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ FEATURE IMPORTANCE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735f8c8",
   "metadata": {},
   "source": [
    "### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP EXPLAINABILITY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š For comprehensive SHAP analysis, see: notebooks/shap_explanability.ipynb\")\n",
    "print(\"\\nSHAP Analysis includes:\")\n",
    "print(\"  1. Summary Plot (Global Feature Importance)\")\n",
    "print(\"  2. Force Plots (Individual Predictions)\")\n",
    "print(\"  3. Dependence Plots (Feature Interactions)\")\n",
    "print(\"  4. Waterfall Charts (Decision Breakdown)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Quick SHAP sample (optional)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"\\nQuick SHAP Summary:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(best_trainer.model)\n",
    "    sample_size = min(100, len(X_test_processed))\n",
    "    shap_values = explainer.shap_values(X_test_processed[:sample_size])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_processed[:sample_size], \n",
    "                      plot_type=\"bar\", show=False, max_display=15)\n",
    "    plt.title(f'SHAP Feature Importance - {best_model}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ SHAP summary generated\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nSHAP not installed\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nSHAP analysis skipped: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93a50d",
   "metadata": {},
   "source": [
    "## 8. Model Persistence & Production Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL TO DISK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models_dir = Path(model_config.models_dir)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = models_dir / f\"creditcard_{best_model_key}_model.joblib\"\n",
    "best_trainer.save_model(str(model_path))\n",
    "print(f\"\\nâœ“ Model Saved:\")\n",
    "print(f\"  Path: {model_path}\")\n",
    "print(f\"  Model: {best_model}\")\n",
    "print(f\"  F1-Score: {best_f1:.4f}\")\n",
    "print(f\"  Recall: {best_recall:.4f}\")\n",
    "print(f\"  AUC-PR: {best_auc_pr:.4f}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor_path = models_dir / \"creditcard_preprocessor.joblib\"\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"\\nâœ“ Preprocessor Saved:\")\n",
    "print(f\"  Path: {preprocessor_path}\")\n",
    "\n",
    "# Save feature engineer\n",
    "feature_engineer_path = models_dir / \"creditcard_feature_engineer.joblib\"\n",
    "joblib.dump(feature_engineer, feature_engineer_path)\n",
    "print(f\"\\nâœ“ Feature Engineer Saved:\")\n",
    "print(f\"  Path: {feature_engineer_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ ALL ARTIFACTS SAVED\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
